library(readr)
library(tm)
library(HMM)
library(stringr)
library(markovchain) 
library(gtools)


cleanText = function(text){
  punct <- '[]\\?!\"\'#$%&(){}+*/:;,._`|~\\[<=>@\\^-]'
  punct2 <- sub( ",.", "", punct )
  newText = gsub(punct2, "", text)
  newText2<-gsub('.',' .',newText,fixed=TRUE)
  newText2<-gsub(',',' ,',newText2,fixed=TRUE)
  newText2<-paste(c('. '),c(newText2))
  return(newText2)
}

cname<-file.path("Adele")
docs <- Corpus(DirSource(cname)) 
# read text into corpus

test<-c()
for (i in 1:10){
  test<-paste(test,docs[[i]]$content,collapse = '[i]')
}
# generate big text chunk

x<-cleanText(test)
x<-tolower(x)
# clean, lower chars

dataframe<-strsplit(as.character(x),split=" ",fixed=TRUE)

dataframe<-unlist(dataframe)

unique<-unique(dataframe)
unique<-unique[unique != ""]
dataframe<-dataframe[dataframe != ""]
# split, find unique, remove ""

mixture<-function(MvS,HMM,MM,text,weights){
  weightmat<-matrix(0,nrow=8,ncol=8)
  # MvS: list of matrices as produced for Mark v Shaney
  # HMM: hmm file as produced with *HMM*
  # weights: c(w1,w2) where w1+w2 = 1 are imput weights 
  # words: length of output matrix
  dataframe<-text
  unique<-unique(dataframe)
  unique<-unique[unique != ""]
  dataframe<-dataframe[dataframe != ""]
  num<-seq(1,length(unique),by=1)
  w1<-weights[1]
  w2<-weights[2]
  w3<-weights[3]
  BW<-HMM
  Tmat<-MM
  xmat<-MvS
  zseq<-seq(0,0,length=length(dataframe))
  zseq[1]<-sample(seq(1,length(BW$hmm$startProbs),by=1),1,prob=BW$hmm$startProbs)
  xseq<-seq(0,0,length=length(dataframe))
  probseq<-seq(0,0,length=length(dataframe))
  xseq[1]<-round(runif(1,1,length(num)))
  xseq[2]<-sample(seq(1,length(num),by=1),size=1,prob = rowSums(xmat[[xseq[1]]]))
  for (i in 2:length(dataframe)){
    zseq[i]<-sample(seq(1,length(BW$hmm$startProbs),by=1),1,replace=FALSE,prob= BW$hmm$transProbs[,zseq[i-1]])
  }
  for (i in 3:length(dataframe)){
    BWprobs<-(BW$hmm$emissionProbs[zseq[i-1],])
    MMprobs<-(Tmat[xseq[i-1],])/sum(Tmat[xseq[i-1],])
    MvSprobs<-(xmat[[xseq[i-2]]][xseq[i-1],])
    xseq[i]<-sample(seq(1,length(unique),by=1),1,replace=FALSE,prob= w1*BWprobs+w2*MMprobs+w3*MvSprobs)
    probseq[i]<-sum(BWprobs)
  }
  return(xseq)
  # note that by default a string with the same length as the input text is generated
}

genMvS<-function(text){
  #text: chr with all words and punctuation seperated as objects
  x<-text
  dataframe<-x
  unique<-unique(dataframe)
  num<-seq(1,length(unique),by=1)
  
  x<-seq(0,0,length=length(dataframe))
  for (i in 1:length(dataframe)){
    x[i]<-num[which(unique==dataframe[i])]
  }
  
  x3<-matrix(0,nrow=length(x)-2,ncol=3)
  for (i in 1:(length(x)-2)){
    x3[i,]<-x[i:(i+2)]
  }
  
  ## new trans matrix:
  
  xmat<-replicate(length(unique), matrix(0,nrow=length(unique),ncol=length(unique)), simplify=F)
  
  for (i in 1:nrow(x3)){
    xmat[[x3[i,1]]][x3[i,2],x3[i,3]]<-xmat[[x3[i,1]]][x3[i,2],x3[i,3]]+1
  }
  
  for (i in 1:length(xmat)){
    for (j in 1:length(xmat)){
      if (sum(xmat[[i]][j,]) > 0){
        xmat[[i]][j,]<-xmat[[i]][j,]/sum(xmat[[i]][j,])
      }
    }
  }
  return(xmat)
  # return nested list of matrices for MvS transitions
}

genHMM<-function(text,maxit,states){
  dataframe<-text
  unique<-unique(dataframe)
  num<-seq(1,length(unique),by=1)
  # maxit: maximum iterations for HMM
  # states: number of states
  # returns BW file as generated by BaumWelch
  
  x<-seq(0,0,length=length(dataframe))
  for (i in 1:length(dataframe)){
    x[i]<-num[which(unique==dataframe[i])]
  }
  gen<-function(m,k){
    pi<-runif(m,0,100)
    startProbs<-pi/sum(pi)
    trans<- matrix(runif(m^2,0,100),nrow=m,ncol=m)
    transProbs<-trans[1:m,]/rowSums(trans)
    phi <- matrix(runif(m*k,0,100),nrow=m,ncol=k)
    emissionProbs<-phi[1:m,]/rowSums(phi)
    vals<-list(startProbs,transProbs,emissionProbs)
  }
  
  vals<-gen(states,length(unique(x)))
  
  
  mtest<-states
  hmm <- initHMM(seq(1,mtest,by=1), seq(1,length(unique(x)),by=1),startProbs=vals[[1]]
                 , transProbs=vals[[2]],emissionProbs =vals[[3]])
  BW<-baumWelch(hmm,observation=x,maxIterations = maxit)
  return(BW)
}

genMM<-function(text){
  unique<-unique(text)
  num<-seq(1,length(unique),by=1)
  #x<-seq(0,0,length=length(dataframe))
  #for (i in 1:length(dataframe)){
  # x[i]<-num[which(unique==dataframe[i])]
  #}
  #dataframe<-x
  #unique<-unique(dataframe)
  #num<-seq(1,length(unique),by=1)
  mcFit <- markovchainFit(data=text,method='mle',byrow=TRUE)
  Tmat.MM<-mcFit$estimat@transitionMatrix
  return(Tmat.MM)
}

splittext<- function(text){
  dataframe<-text[1:(floor(length(text)/2))]
  dataframe.test<-text[(floor(length(text)/2)+1):length(text)]
  unique<-unique(dataframe)
  unique<-unique[unique != ""]
  dataframe<-dataframe[dataframe != ""]
  unique.test<-unique(dataframe.test)
  unique.test<-unique.test[unique.test != ""]
  dataframe.test<-dataframe.test[dataframe.test != ""]
  return(list(dataframe,dataframe.test))
}

combine<-function(text,maxit,states){
  test.text<-splittext(text)
  xmat<-genMvS(test.text[[1]])
  BW<-genHMM(test.text[[1]],maxit,states)
  MM<-genMM(test.text[[1]])
  xmat.2<-genMvS(test.text[[2]])
  BW.2<-genHMM(test.text[[2]],maxit,states)
  MM.2<-genMM(test.text[[2]])
  return(list(list(test.text[[1]],xmat,BW,MM),list(test.text[[2]],xmat.2,BW.2,MM.2)))
  # This is a list of two lists. The first part is the "training" half of the text, and the second part for the "test" half
  # note that we split the original text down the center
  # both part contain a list of all components needed for mixture. 
}

mixture.train<-function(MvS,HMM,MM,text.train,text.test,weights){
  weightmat<-matrix(0,nrow=8,ncol=8)
  # MvS: list of matrices as produced for Mark v Shaney
  # HMM: hmm file as produced with *HMM*
  # weights: c(w1,w2) where w1+w2 = 1 are imput weights 
  # words: length of output matrix
  dataframe<-text.train
  dataframe.test<-text.test
  unique<-unique(dataframe)
  unique<-unique[unique != ""]
  unique.test<-unique(dataframe.test)
  unique.test<-unique.test[unique.test != ""]
  num<-seq(1,length(unique),by=1)
  w1<-weights[1]
  w2<-weights[2]
  w3<-weights[3]
  BW<-HMM
  Tmat<-MM
  xmat<-MvS
  zseq<-seq(0,0,length=length(dataframe))
  zseq[1]<-sample(seq(1,length(BW$hmm$startProbs),by=1),1,prob=BW$hmm$startProbs)
  xseq<-seq(0,0,length=length(dataframe))
  probseq<-seq(0,0,length=length(dataframe))
  xseq[1]<-round(runif(1,1,length(num)))
  xseq[2]<-sample(seq(1,length(num),by=1),size=1,prob = rowSums(xmat[[xseq[1]]]))
  for (i in 2:length(dataframe)){
    zseq[i]<-sample(seq(1,length(BW$hmm$startProbs),by=1),1,replace=FALSE,prob= BW$hmm$transProbs[,zseq[i-1]])
  }
  for (i in 3:length(dataframe)){
    BWprobs<-(BW$hmm$emissionProbs[zseq[i-1],])
    MMprobs<-(Tmat[xseq[i-1],])/sum(Tmat[xseq[i-1],])
    MvSprobs<-(xmat[[xseq[i-2]]][xseq[i-1],])
    xseq[i]<-sample(seq(1,length(unique),by=1),1,replace=FALSE,prob= w1*BWprobs+w2*MMprobs+w3*MvSprobs)
    probseq[i]<-sum(BWprobs)
  }
  # note that by default a string with the same length as the input text is generated
  truerate<-scale(c(sort(table(dataframe.test),decreasing=TRUE)[1:20]))
  rate<-scale(c(sort(table(xseq),decreasing=TRUE)[1:20]))
  return(sum(abs(rate-truerate)))
}


test.3<-combine(dataframe,10,10)


xmat<-test.3[[1]][[2]]
BW<-test.3[[1]][[3]]
MM<-test.3[[1]][[4]]
# data using to generate # 
text.test<-test.3[[1]][[1]]
# holdout #  
text.train <- test.3[[2]][[1]]
# generated text # 
gen.text <- mixture(xmat, BW, MM, text.test, weights = c(0.2, 0.4, 0.4))
gen.text <- as.vector(unique[gen.text])
gen.text <- paste(gen.text,sep=" ", collapse = " ")


sep.sentences <- function(text){
  sents <- paste(text, collapse = " ")
  sents <- strsplit(sents, "[.]")
  sents <-sapply(sents, str_trim)
  length.sents <- nchar(sents)
  nums <- unique(sort(length.sents, decreasing = FALSE))
  nums <- nums[nums!=0]
  sents2 <- list()
  for(i in 1:length(nums)){
    sents2[[i]] <- unique(sents[which(length.sents == nums[i])])
  }
  names(sents2) <- nums
  
  return(sents2)
}

holdout <- sep.sentences(text.train)
orig <- sep.sentences(text.test)
gen <- sep.sentences(gen.text)

close.match <- function(comp, orig){
  comp.num <- as.numeric(names(comp))
  orig.num <- as.numeric(names(orig))
  min.value <- c()
  for(i in 1:length(orig)){
    min.value[i] <- which.min(abs(orig.num[i]- comp.num))
  }
  dist.value <- c()
  for(i in 1:length(orig)){
    dist.value[i] <- mean(adist(orig[[i]], comp[[min.value[i]]]))
  }
  return(dist.value)
}

close.match(gen, orig)


# bw, mm, mvs # 

#[1]   8.00000  15.00000  19.00000  24.00000  24.66667  24.66667  28.75000  33.00000  39.00000  39.00000  40.00000  55.50000  82.00000 164.00000 178.00000
#[16] 342.00000 373.00000 430.00000 weights = 1/3, 1/3, 1/3

#10.0  14.0  20.0  22.0  23.0  22.0  39.5  38.0  41.0  47.0  44.0  46.0  81.0 167.0 189.0 349.0 403.0 447.0 weights = 1, 0, 0

#11.0  16.5  19.0  22.0  25.0  25.0  31.0  33.0  38.0  43.0  40.0  54.0  83.0 189.0 191.0 371.0 396.0 460.0 weights = 0, 1, 0 

#0.0   0.0  21.0   0.0   0.0   0.0  15.5   0.0   0.0  43.0   0.0   0.0  88.0 179.0 155.0 186.0 292.0 339.0 weights = 0, 0, 1

#  [1]  13.00000  14.00000  21.00000  23.00000  22.00000  24.00000  27.50000  32.00000  43.00000  47.33333  45.00000  53.00000  86.00000 180.00000 194.00000
# [16] 370.00000 396.00000 453.00000 weights = 0.2, 0.4, 0.4

# alpha # 
wordcompare<-function(seq1,seq2,topwords){
  truerate<-scale(c(sort(table(seq1),decreasing=TRUE)[1:topwords]))
  rate<-scale(c(sort(table(seq2),decreasing=TRUE)[1:topwords]))
  return(sum(abs(rate-truerate)))
}
# gen.seq comes from mixture
# second argument any vector of words or numbers 
# third argument how many most frequent words we want to consider


numtotext<-function(seq1,unique){
  gen.text <- as.vector(unique[seq1])
  return(gen.text)
}
# takes argument vector of numbers and text chunk (non-holdout)
# returns concatenated text chunk



# BW, MM, MVS #
metropolis <- function(weights, MvS, HMM, MM, test.text, train.text, iter){
  
  weight.vec <- matrix(nrow = 3, ncol = iter)
  weight.vec[,1] <- weights
  
  # alpha # 
  dist <- c()
  
  
  
  seq.gen <-mixture(MvS, HMM, MM, text.test, weight.vec[,1])
  dist[1] <- wordcompare(seq.gen,text.train,topwords=20)
  
  
  
  holdout <- sep.sentences(text.train)
  orig <- sep.sentences(text.test)
  
  for(i in 2:iter){
    seq.gen <- mixture(MvS, HMM, MM, text.test, weight.vec[,i-1])
    dist.old <-wordcompare(seq.gen,text.train,topwords=20)
    gen.text <- as.vector(unique[seq.gen])
    gen.text <- paste(gen.text,sep=" ", collapse = " ")
    gen <- sep.sentences(gen.text)
    beta.old <- close.match(gen, orig)
    delta.old <- sum(beta.old ==0)/length(beta.old)    
    
    new.weights <- rdirichlet(1, weight.vec[,i-1])
    new.weights <- new.weights/sum(new.weights)
    

    seq.gen <- mixture(MvS, HMM, MM, text.test, new.weights)
    dist.new <-wordcompare(seq.gen,text.train,topwords=20)  
    gen.text <- as.vector(unique[seq.gen])
    gen.text <- paste(gen.text,sep=" ", collapse = " ")
    gen <- sep.sentences(gen.text)
    beta.new <- close.match(gen, orig)
    delta.new <- sum(beta.new == 0)/length(beta.new)

    
    gamma.old <- dist.old 
    gamma.new <- dist.new 
    
    if(gamma.old < gamma.new){
      weight.vec[,i] <- new.weights
    }
    else(weight.vec[,i] <- weight.vec[,i-1])
    seq.gen<- mixture(MvS, HMM, MM, text.test, weight.vec[,i])
    dist[i]<-wordcompare(seq.gen,text.train,topwords=20)
  }
  
  return(list(t(weight.vec), dist))
  
}






